{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "display(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './'\n",
    "BATCH_SIZE = 32\n",
    "STEPS = 2000\n",
    "DROPOUT = 0.1\n",
    "HIDDEN_UNITS = [64, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup methods to be re-used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5b4a1ad54b63aa3df95c141b5fa902c27ae2233b"
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    \"\"\"Load data from a given file.\n",
    "    \n",
    "    Args:\n",
    "        file - a string path that will be appended to BASE_DIR\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    import json as json\n",
    "    with open(BASE_DIR + file) as f:\n",
    "        json_data = json.load(f)\n",
    "    data = pd.DataFrame(json_data)\n",
    "    display(len(json_data))\n",
    "    display(json_data[0])\n",
    "    display(data.head())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(data):\n",
    "    \"\"\"Create the unique mappings based upon the data passed in\n",
    "    \n",
    "    Args:\n",
    "        data - a pandas.DataFrame with cuisine and ingredients columns\n",
    "    \n",
    "    Returns:\n",
    "        ingredients_mapping - a dict with key being the string and a unique int\n",
    "        cuisine_mapping - a dict with key being the string and a unique int\n",
    "    \"\"\"\n",
    "    unique_cuisines = data['cuisine'].unique()\n",
    "    cuisine_mapping = {value: idx for idx, value in enumerate(unique_cuisines)}\n",
    "    unique_ingredients = []\n",
    "    for idx, d in data.iterrows(): \n",
    "        unique_ingredients += d['ingredients']\n",
    "    unique_ingredients = set(unique_ingredients)\n",
    "    ingredients_mapping = {value: idx for idx, value in enumerate(unique_ingredients)}\n",
    "    return ingredients_mapping, cuisine_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dense_array(data, cuisine_mapping, ingredients_mapping, shape=None, make_labels=True):\n",
    "    \"\"\"Transforms data to a dense array to be used in the model.\n",
    "    \n",
    "    This method basically turns a sparse representation into a dense representation of the data.\n",
    "    \n",
    "    Args:\n",
    "        data - a pandas.DataFrame\n",
    "        cuisine_mapping - the mapping derived from get_mappings\n",
    "        ingredients_mapping - the mapping derived from get_mappings\n",
    "        \n",
    "    Kwargs:\n",
    "        shape - optional max number of rows\n",
    "        make_labels - create labels for each of the rows\n",
    "    \n",
    "    Returns:\n",
    "        transformed - the transformed data to be used in the model. The columns are the values from\n",
    "            ingredients_mapping.\n",
    "        labels - the labels corresponding to the rows (is an empty array if make_labels=False)\n",
    "        data - original data but with the index reset\n",
    "    \"\"\"\n",
    "    if shape is None:\n",
    "        shape = len(data)\n",
    "    transformed = {str(k): np.zeros(shape) for k in ingredients_mapping.values()}\n",
    "    labels = []\n",
    "    data = data.reset_index(drop=True)\n",
    "    for idx, d in data.iterrows():\n",
    "        if make_labels:\n",
    "            labels.append(cuisine_mapping[d['cuisine']])\n",
    "        for ing in d['ingredients']:\n",
    "            transformed[str(ingredients_mapping[ing])][idx] += 1\n",
    "        if idx >= shape - 1:\n",
    "            break\n",
    "    assert len(transformed['0']) == len(labels)\n",
    "    return transformed, labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_columns(ingredients_mapping):\n",
    "    \"\"\"Creates the columns for the models based upon ingredient_mapping.values()\n",
    "    \n",
    "    Args:\n",
    "        ingredients_mapping - the mapping gotten from get_mappings\n",
    "    \n",
    "    Returns:\n",
    "        An array of columns.\n",
    "    \"\"\"\n",
    "    return [tf.feature_column.numeric_column(str(x)) for x in ingredients_mapping.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "229c32157afa637def0fc82a3b61111dcb819ff2"
   },
   "outputs": [],
   "source": [
    "def train_input_fn(features, label_list, batch_size):\n",
    "    \"\"\"An input function used to get the data for training the model.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), label_list))\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, label_list, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if label_list is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = (features,)\n",
    "    else:\n",
    "        inputs = (features, label_list)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the program\n",
    "\n",
    "For this we are going to use the provided training data for both training and eval.\n",
    "\n",
    "Notes:\n",
    "  - Because of memory limitations we use only 10,000 records for training, and 10,000 records for eval. The problem appears to be loading of all the dataset into memory. An improvement would be to look into streaming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8b1848b85ea6a68d55ca128ae5ce9ddc598192f9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 10259,\n",
       " 'cuisine': 'greek',\n",
       " 'ingredients': ['romaine lettuce',\n",
       "  'black olives',\n",
       "  'grape tomatoes',\n",
       "  'garlic',\n",
       "  'pepper',\n",
       "  'purple onion',\n",
       "  'seasoning',\n",
       "  'garbanzo beans',\n",
       "  'feta cheese crumbles']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = get_data('train.json')\n",
    "ingredients_mapping, cuisine_mapping = get_mappings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "30ffcd60cab3d83f785beb5ffbd55cb636e0ea50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data, training_labels, _ = transform_to_dense_array(\n",
    "    data[0:10_000], cuisine_mapping, ingredients_mapping)\n",
    "\n",
    "eval_data, eval_labels, _ = transform_to_dense_array(\n",
    "    data[10_001:20_000], cuisine_mapping, ingredients_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3a685ef262bff70ddb182b8ed398e4c54cf5c1d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_NumericColumn(key='0', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_columns = make_columns(ingredients_mapping)\n",
    "display(all_columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Here we are using the DNNClassifier to train the model. During this run it was 73% accurate, which isn't great but not too bad either (for the first time using tensorflow). See https://www.kaggle.com/c/whats-cooking-kernels-only for other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c3b008695db89cca7b1ec94c85fc9ba2f59c0a47",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /home/jovyan/tmpd4xkmexs\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jovyan/tmpd4xkmexs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb89596c3c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/jovyan/tmpd4xkmexs/model.ckpt.\n",
      "INFO:tensorflow:loss = 95.839264, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.03793\n",
      "INFO:tensorflow:loss = 37.954765, step = 101 (96.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.85491\n",
      "INFO:tensorflow:loss = 47.403023, step = 201 (20.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.25365\n",
      "INFO:tensorflow:loss = 34.786507, step = 301 (19.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.46216\n",
      "INFO:tensorflow:loss = 13.772985, step = 401 (22.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.77287\n",
      "INFO:tensorflow:loss = 18.211582, step = 501 (20.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.85123\n",
      "INFO:tensorflow:loss = 22.61219, step = 601 (20.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.52684\n",
      "INFO:tensorflow:loss = 18.599836, step = 701 (22.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.82824\n",
      "INFO:tensorflow:loss = 14.183556, step = 801 (20.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.64615\n",
      "INFO:tensorflow:loss = 21.557747, step = 901 (21.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.23928\n",
      "INFO:tensorflow:loss = 19.242012, step = 1001 (23.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.46305\n",
      "INFO:tensorflow:loss = 17.274086, step = 1101 (22.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.62774\n",
      "INFO:tensorflow:loss = 5.0775766, step = 1201 (21.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.50587\n",
      "INFO:tensorflow:loss = 8.546097, step = 1301 (22.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.50502\n",
      "INFO:tensorflow:loss = 8.935773, step = 1401 (22.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.40025\n",
      "INFO:tensorflow:loss = 6.5940113, step = 1501 (22.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.63515\n",
      "INFO:tensorflow:loss = 7.0553107, step = 1601 (21.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.35535\n",
      "INFO:tensorflow:loss = 4.7844477, step = 1701 (22.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.2718\n",
      "INFO:tensorflow:loss = 7.646406, step = 1801 (23.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.08249\n",
      "INFO:tensorflow:loss = 9.59461, step = 1901 (24.492 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /home/jovyan/tmpd4xkmexs/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.1335754.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7fb895a53f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=all_columns,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    n_classes=len(cuisine_mapping.keys()),\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "estimator.train(\n",
    "    input_fn=lambda: train_input_fn(training_data, training_labels, BATCH_SIZE),\n",
    "    steps=STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-07-16:27:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/tmpd4xkmexs/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-07-16:29:59\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.73717374, average_loss = 1.0873905, global_step = 2000, loss = 34.73744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test set accuracy: 0.737'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result = estimator.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(eval_data, eval_labels, BATCH_SIZE)\n",
    ")\n",
    "\n",
    "display('Test set accuracy: {accuracy:0.3f}'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.73717374,\n",
       " 'average_loss': 1.0873905,\n",
       " 'loss': 34.73744,\n",
       " 'global_step': 2000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
